{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Torpedo2648/AI-Introduction/blob/main/scenic/projects/owl_vit/notebooks/OWL_ViT_minimal_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaiG8Ulc75xc"
      },
      "source": [
        "# OWL-ViT minimal example\n",
        "\n",
        "This Colab shows how to **load a pre-trained OWL-ViT checkpoint** and use it to\n",
        "**get object detection predictions** for an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-Yta1B7rtWu"
      },
      "source": [
        "# Download and install OWL-ViT\n",
        "\n",
        "OWL-ViT is implemented in [Scenic](https://github.com/google-research/scenic). The cell below installs the Scenic codebase from GitHub and imports it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zWF7RkeZ4B_N",
        "outputId": "06b450ca-9fe7-414e-c3a7-13dbc175dcbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '.'...\n",
            "remote: Enumerating objects: 9141, done.\u001b[K\n",
            "remote: Counting objects: 100% (1762/1762), done.\u001b[K\n",
            "remote: Compressing objects: 100% (381/381), done.\u001b[K\n",
            "remote: Total 9141 (delta 1464), reused 1510 (delta 1381), pack-reused 7379 (from 1)\u001b[K\n",
            "Receiving objects: 100% (9141/9141), 62.10 MiB | 21.44 MiB/s, done.\n",
            "Resolving deltas: 100% (6592/6592), done.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for scenic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for optax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/openai/CLIP.git (from -r ./scenic/projects/owl_vit/requirements.txt (line 4))\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-73o_cl6g\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-73o_cl6g\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI (from -r ./scenic/projects/owl_vit/requirements.txt (line 7))\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-vih_nmi2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-vih_nmi2\n",
            "  Resolved https://github.com/cocodataset/cocoapi.git to commit 8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.10.2 in /usr/local/lib/python3.10/dist-packages (from -r ./scenic/projects/owl_vit/requirements.txt (line 2)) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r ./scenic/projects/owl_vit/requirements.txt (line 3)) (4.66.6)\n",
            "Collecting lvis (from -r ./scenic/projects/owl_vit/requirements.txt (line 8))\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl.metadata (856 bytes)\n",
            "Collecting ott-jax<0.4.0 (from -r ./scenic/projects/owl_vit/requirements.txt (line 11))\n",
            "  Downloading ott_jax-0.3.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->-r ./scenic/projects/owl_vit/requirements.txt (line 2)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->-r ./scenic/projects/owl_vit/requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->-r ./scenic/projects/owl_vit/requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->-r ./scenic/projects/owl_vit/requirements.txt (line 2)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->-r ./scenic/projects/owl_vit/requirements.txt (line 2)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.2->-r ./scenic/projects/owl_vit/requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.2->-r ./scenic/projects/owl_vit/requirements.txt (line 2)) (1.3.0)\n",
            "Collecting ftfy (from clip==1.0->-r ./scenic/projects/owl_vit/requirements.txt (line 4))\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0->-r ./scenic/projects/owl_vit/requirements.txt (line 4)) (24.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0->-r ./scenic/projects/owl_vit/requirements.txt (line 4)) (2024.9.11)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0->-r ./scenic/projects/owl_vit/requirements.txt (line 4)) (0.20.0+cu121)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0->-r ./scenic/projects/owl_vit/requirements.txt (line 7)) (75.1.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0->-r ./scenic/projects/owl_vit/requirements.txt (line 7)) (3.0.11)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0->-r ./scenic/projects/owl_vit/requirements.txt (line 7)) (3.8.0)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lvis->-r ./scenic/projects/owl_vit/requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from lvis->-r ./scenic/projects/owl_vit/requirements.txt (line 8)) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from lvis->-r ./scenic/projects/owl_vit/requirements.txt (line 8)) (1.26.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from lvis->-r ./scenic/projects/owl_vit/requirements.txt (line 8)) (4.10.0.84)\n",
            "Requirement already satisfied: pyparsing>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from lvis->-r ./scenic/projects/owl_vit/requirements.txt (line 8)) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from lvis->-r ./scenic/projects/owl_vit/requirements.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from lvis->-r ./scenic/projects/owl_vit/requirements.txt (line 8)) (1.16.0)\n",
            "Requirement already satisfied: jax>=0.1.67 in /usr/local/lib/python3.10/dist-packages (from ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (0.4.33)\n",
            "Requirement already satisfied: jaxlib>=0.1.47 in /usr/local/lib/python3.10/dist-packages (from ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (0.4.33)\n",
            "Requirement already satisfied: flax>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (0.8.5)\n",
            "Requirement already satisfied: optax>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (0.2.4.dev0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (1.13.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.5.2->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.5.2->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (0.6.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.5.2->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (0.1.67)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.5.2->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (13.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.5.2->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.67->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.67->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0->-r ./scenic/projects/owl_vit/requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0->-r ./scenic/projects/owl_vit/requirements.txt (line 7)) (4.54.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0->-r ./scenic/projects/owl_vit/requirements.txt (line 7)) (10.4.0)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax>=0.1.1->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.10/dist-packages (from optax>=0.1.1->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (0.1.87)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.10/dist-packages (from optax>=0.1.1->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (1.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0->-r ./scenic/projects/owl_vit/requirements.txt (line 4)) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.2->-r ./scenic/projects/owl_vit/requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.87->optax>=0.1.1->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (0.12.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.5.2->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.5.2->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (2.18.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.5.2->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.5.2->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (3.20.3)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.5.2->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (4.11.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.5.2->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (0.1.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epy]->optax>=0.1.1->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epy]->optax>=0.1.1->ott-jax<0.4.0->-r ./scenic/projects/owl_vit/requirements.txt (line 11)) (3.20.2)\n",
            "Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Downloading ott_jax-0.3.1-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip, pycocotools\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=ead8a7890e999d106f495f4beabec45dfef2d0b06b4213e4cf0b7e5111cb9863\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h96s8khm/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp310-cp310-linux_x86_64.whl size=376060 sha256=05d14c0d3c49e765e1eccfeb82b102cb42fac0267b66fc24f1d0feaa3332ee49\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h96s8khm/wheels/39/61/b4/480fbddb4d3d6bc34083e7397bc6f5d1381f79acc68e9f3511\n",
            "Successfully built clip pycocotools\n",
            "Installing collected packages: ftfy, pycocotools, lvis, clip, ott-jax\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.8\n",
            "    Uninstalling pycocotools-2.0.8:\n",
            "      Successfully uninstalled pycocotools-2.0.8\n",
            "Successfully installed clip-1.0 ftfy-6.3.1 lvis-0.5.3 ott-jax-0.3.1 pycocotools-2.0\n",
            "Cloning into '/big_vision'...\n",
            "remote: Enumerating objects: 1348, done.\u001b[K\n",
            "remote: Counting objects: 100% (859/859), done.\u001b[K\n",
            "remote: Compressing objects: 100% (384/384), done.\u001b[K\n",
            "remote: Total 1348 (delta 577), reused 577 (delta 441), pack-reused 489 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1348/1348), 4.84 MiB | 10.41 MiB/s, done.\n",
            "Resolving deltas: 100% (754/754), done.\n",
            "Collecting git+https://github.com/google/CommonLoopUtils (from -r /big_vision/big_vision/requirements.txt (line 3))\n",
            "  Cloning https://github.com/google/CommonLoopUtils to /tmp/pip-req-build-055sl_rr\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/google/CommonLoopUtils /tmp/pip-req-build-055sl_rr\n",
            "  Resolved https://github.com/google/CommonLoopUtils to commit 307b0bc65ae2e2d801b6c19df2431c060a0aa4ec\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/google/flaxformer (from -r /big_vision/big_vision/requirements.txt (line 9))\n",
            "  Cloning https://github.com/google/flaxformer to /tmp/pip-req-build-d1r_2l_2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/google/flaxformer /tmp/pip-req-build-d1r_2l_2\n",
            "  Resolved https://github.com/google/flaxformer to commit 399ea3a85e9807ada653fd0de1a9de627eb0acde\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/akolesnikoff/panopticapi.git@mute (from -r /big_vision/big_vision/requirements.txt (line 10))\n",
            "  Cloning https://github.com/akolesnikoff/panopticapi.git (to revision mute) to /tmp/pip-req-build-mw6wx0es\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/akolesnikoff/panopticapi.git /tmp/pip-req-build-mw6wx0es\n",
            "  Running command git checkout -b mute --track origin/mute\n",
            "  Switched to a new branch 'mute'\n",
            "  Branch 'mute' set up to track remote branch 'mute' from 'origin'.\n",
            "  Resolved https://github.com/akolesnikoff/panopticapi.git to commit a698a12deb21e4cf0f99ef0581b2c30c466bf355\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.10/dist-packages (from -r /big_vision/big_vision/requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from -r /big_vision/big_vision/requirements.txt (line 2)) (1.4.0)\n",
            "Collecting distrax (from -r /big_vision/big_vision/requirements.txt (line 4))\n",
            "  Downloading distrax-0.1.5-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from -r /big_vision/big_vision/requirements.txt (line 5)) (0.8.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from -r /big_vision/big_vision/requirements.txt (line 6)) (0.8.0)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (from -r /big_vision/big_vision/requirements.txt (line 7)) (0.8.5)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from -r /big_vision/big_vision/requirements.txt (line 8)) (0.2.4.dev0)\n",
            "Collecting overrides (from -r /big_vision/big_vision/requirements.txt (line 11))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r /big_vision/big_vision/requirements.txt (line 12)) (3.20.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r /big_vision/big_vision/requirements.txt (line 13)) (0.2.0)\n",
            "Collecting tensorflow-cpu (from -r /big_vision/big_vision/requirements.txt (line 14))\n",
            "  Downloading tensorflow_cpu-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tfds-nightly (from -r /big_vision/big_vision/requirements.txt (line 15))\n",
            "  Downloading tfds_nightly-4.9.7.dev202411030045-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting tensorflow-text (from -r /big_vision/big_vision/requirements.txt (line 16))\n",
            "  Downloading tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-gan (from -r /big_vision/big_vision/requirements.txt (line 17))\n",
            "  Downloading tensorflow_gan-2.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pycocoevalcap (from -r /big_vision/big_vision/requirements.txt (line 18))\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from clu==0.0.12->-r /big_vision/big_vision/requirements.txt (line 3)) (1.10.0)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from clu==0.0.12->-r /big_vision/big_vision/requirements.txt (line 3)) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from clu==0.0.12->-r /big_vision/big_vision/requirements.txt (line 3)) (0.4.33)\n",
            "Requirement already satisfied: ml_collections in /usr/local/lib/python3.10/dist-packages (from clu==0.0.12->-r /big_vision/big_vision/requirements.txt (line 3)) (0.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clu==0.0.12->-r /big_vision/big_vision/requirements.txt (line 3)) (24.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from clu==0.0.12->-r /big_vision/big_vision/requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from clu==0.0.12->-r /big_vision/big_vision/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: chex>=0.1.8 in /usr/local/lib/python3.10/dist-packages (from distrax->-r /big_vision/big_vision/requirements.txt (line 4)) (0.1.87)\n",
            "Requirement already satisfied: tensorflow-probability>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from distrax->-r /big_vision/big_vision/requirements.txt (line 4)) (0.24.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax->-r /big_vision/big_vision/requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax->-r /big_vision/big_vision/requirements.txt (line 7)) (0.6.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax->-r /big_vision/big_vision/requirements.txt (line 7)) (0.1.67)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax->-r /big_vision/big_vision/requirements.txt (line 7)) (13.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax->-r /big_vision/big_vision/requirements.txt (line 7)) (6.0.2)\n",
            "Collecting aqtp>=0.1.0 (from flaxformer==0.8.8->-r /big_vision/big_vision/requirements.txt (line 9))\n",
            "  Downloading aqtp-0.8.2-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from panopticapi==0.1->-r /big_vision/big_vision/requirements.txt (line 10)) (10.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (3.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (2.5.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (1.64.1)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14))\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14))\n",
            "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (0.37.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->-r /big_vision/big_vision/requirements.txt (line 15)) (0.1.8)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->-r /big_vision/big_vision/requirements.txt (line 15)) (4.2.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->-r /big_vision/big_vision/requirements.txt (line 15)) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->-r /big_vision/big_vision/requirements.txt (line 15)) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->-r /big_vision/big_vision/requirements.txt (line 15)) (17.0.0)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->-r /big_vision/big_vision/requirements.txt (line 15)) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->-r /big_vision/big_vision/requirements.txt (line 15)) (1.16.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->-r /big_vision/big_vision/requirements.txt (line 15)) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->-r /big_vision/big_vision/requirements.txt (line 15)) (4.66.6)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tfds-nightly->-r /big_vision/big_vision/requirements.txt (line 15)) (0.5.1)\n",
            "Collecting tensorflow<2.19,>=2.18.0 (from tensorflow-text->-r /big_vision/big_vision/requirements.txt (line 16))\n",
            "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-gan->-r /big_vision/big_vision/requirements.txt (line 17)) (0.16.1)\n",
            "Collecting pycocotools>=2.0.2 (from pycocoevalcap->-r /big_vision/big_vision/requirements.txt (line 18))\n",
            "  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (0.44.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.8->distrax->-r /big_vision/big_vision/requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tfds-nightly->-r /big_vision/big_vision/requirements.txt (line 15)) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tfds-nightly->-r /big_vision/big_vision/requirements.txt (line 15)) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tfds-nightly->-r /big_vision/big_vision/requirements.txt (line 15)) (3.20.2)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->clu==0.0.12->-r /big_vision/big_vision/requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (0.13.0)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap->-r /big_vision/big_vision/requirements.txt (line 18)) (3.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax->-r /big_vision/big_vision/requirements.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax->-r /big_vision/big_vision/requirements.txt (line 7)) (2.18.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (3.0.6)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.2->tensorflow-gan->-r /big_vision/big_vision/requirements.txt (line 17)) (2.17.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax->-r /big_vision/big_vision/requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax->-r /big_vision/big_vision/requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml_collections->clu==0.0.12->-r /big_vision/big_vision/requirements.txt (line 3)) (21.6.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->-r /big_vision/big_vision/requirements.txt (line 7)) (1.6.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->-r /big_vision/big_vision/requirements.txt (line 7)) (4.11.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tfds-nightly->-r /big_vision/big_vision/requirements.txt (line 15)) (0.16)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->-r /big_vision/big_vision/requirements.txt (line 7)) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->-r /big_vision/big_vision/requirements.txt (line 18)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->-r /big_vision/big_vision/requirements.txt (line 18)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->-r /big_vision/big_vision/requirements.txt (line 18)) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->-r /big_vision/big_vision/requirements.txt (line 18)) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->-r /big_vision/big_vision/requirements.txt (line 18)) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->-r /big_vision/big_vision/requirements.txt (line 18)) (2.8.2)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub>=0.2->tensorflow-gan->-r /big_vision/big_vision/requirements.txt (line 17))\n",
            "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-cpu->-r /big_vision/big_vision/requirements.txt (line 14)) (3.0.2)\n",
            "Downloading distrax-0.1.5-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading tensorflow_cpu-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (230.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.0/230.0 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tfds_nightly-4.9.7.dev202411030045-py3-none-any.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_gan-2.1.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.1/367.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aqtp-0.8.2-py3-none-any.whl (894 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.1/894.1 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.8/427.8 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flaxformer, panopticapi\n",
            "  Building wheel for flaxformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flaxformer: filename=flaxformer-0.8.8-py3-none-any.whl size=323605 sha256=a4a0ff11d2caaf3d32d4cad733d6c290daf04f039bd7401e0199dcdbe2c54b6c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tzx1fwcl/wheels/92/28/c3/a5ec203c08082d1e7297c97ed4519688c6fa12a73c84052cca\n",
            "  Building wheel for panopticapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for panopticapi: filename=panopticapi-0.1-py3-none-any.whl size=16969 sha256=d9cd702dbab93310303309a4f56ebfe1d966e1b35031aa5c73cf5f6a304601a3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tzx1fwcl/wheels/7f/22/5f/df8b799000f3c073d37a4622bba554792e43cdcea8199f535e\n",
            "Successfully built flaxformer panopticapi\n",
            "Installing collected packages: panopticapi, overrides, tensorboard, pycocotools, keras, tensorflow-cpu, tensorflow, pycocoevalcap, tfds-nightly, tf-keras, tensorflow-text, distrax, tensorflow-gan, aqtp, flaxformer\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0\n",
            "    Uninstalling pycocotools-2.0:\n",
            "      Successfully uninstalled pycocotools-2.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.17.0\n",
            "    Uninstalling tf_keras-2.17.0:\n",
            "      Successfully uninstalled tf_keras-2.17.0\n",
            "Successfully installed aqtp-0.8.2 distrax-0.1.5 flaxformer-0.8.8 keras-3.6.0 overrides-7.7.0 panopticapi-0.1 pycocoevalcap-1.2 pycocotools-2.0.8 tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-cpu-2.18.0 tensorflow-gan-2.1.0 tensorflow-text-2.18.0 tf-keras-2.18.0 tfds-nightly-4.9.7.dev202411030045\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf *\n",
        "!rm -rf .config\n",
        "!rm -rf .git\n",
        "!git clone https://github.com/google-research/scenic.git .\n",
        "!python -m pip install -q .\n",
        "!python -m pip install -r ./scenic/projects/owl_vit/requirements.txt\n",
        "\n",
        "# Also install big_vision, which is needed for the mask head:\n",
        "!mkdir /big_vision\n",
        "!git clone https://github.com/google-research/big_vision.git /big_vision\n",
        "!python -m pip install -r /big_vision/big_vision/requirements.txt\n",
        "import sys\n",
        "sys.path.append('/big_vision/')\n",
        "!echo \"Done.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install jax==0.4.27 jaxlib==0.4.27\n",
        "import sys\n",
        "sys.path.append('/big_vision/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWNd0U6u7LSa",
        "outputId": "39e74621-5e0b-45e3-ac63-014e62b0bf85"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax==0.4.27 in /usr/local/lib/python3.10/dist-packages (0.4.27)\n",
            "Requirement already satisfied: jaxlib==0.4.27 in /usr/local/lib/python3.10/dist-packages (0.4.27)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.27) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.27) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.27) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.27) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MKZb6G3-H92"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('/big_vision/')\n",
        "\n",
        "import jax\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from scenic.projects.owl_vit import configs\n",
        "from scenic.projects.owl_vit import models\n",
        "from scipy.special import expit as sigmoid\n",
        "import skimage\n",
        "from skimage import io as skimage_io\n",
        "from skimage import transform as skimage_transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WatINO87evx"
      },
      "source": [
        "# Choose config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4RKu3Vv5k_3"
      },
      "outputs": [],
      "source": [
        "config = configs.owl_v2_clip_b16.get_config(init_mode='canonical_checkpoint')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c12cyRK7oOD"
      },
      "source": [
        "# Load the model and variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s421Kpp7sXjD"
      },
      "outputs": [],
      "source": [
        "module = models.TextZeroShotDetectionModule(\n",
        "    body_configs=config.model.body,\n",
        "    objectness_head_configs=config.model.objectness_head,\n",
        "    normalize=config.model.normalize,\n",
        "    box_bias=config.model.box_bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmaY8tQ23nJ3"
      },
      "outputs": [],
      "source": [
        "variables = module.load_variables(config.init_from.checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Knbjoxy2zW"
      },
      "source": [
        "# Prepare image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99ilV_T2RyNT"
      },
      "outputs": [],
      "source": [
        "# Load example image:\n",
        "filename = os.path.join(skimage.data_dir, 'astronaut.png')\n",
        "image_uint8 = skimage_io.imread(filename)\n",
        "image = image_uint8.astype(np.float32) / 255.0\n",
        "\n",
        "# Pad to square with gray pixels on bottom and right:\n",
        "h, w, _ = image.shape\n",
        "size = max(h, w)\n",
        "image_padded = np.pad(\n",
        "    image, ((0, size - h), (0, size - w), (0, 0)), constant_values=0.5)\n",
        "\n",
        "# Resize to model input size:\n",
        "input_image = skimage.transform.resize(\n",
        "    image_padded,\n",
        "    (config.dataset_configs.input_size, config.dataset_configs.input_size),\n",
        "    anti_aliasing=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJvG0eaYyplV"
      },
      "source": [
        "# Prepare text queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSDsqV0UxbtL"
      },
      "outputs": [],
      "source": [
        "text_queries = ['face', 'rocket', 'nasa badge', 'star-spangled banner']\n",
        "tokenized_queries = np.array([\n",
        "    module.tokenize(q, config.dataset_configs.max_query_length)\n",
        "    for q in text_queries\n",
        "])\n",
        "\n",
        "# Pad tokenized queries to avoid recompilation if number of queries changes:\n",
        "tokenized_queries = np.pad(\n",
        "    tokenized_queries,\n",
        "    pad_width=((0, 100 - len(text_queries)), (0, 0)),\n",
        "    constant_values=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdR3OpAIzAA0"
      },
      "source": [
        "# Get predictions\n",
        "This will take a minute on the first execution due to model compilation. Subsequent executions will be faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkU2rMjTrjtK"
      },
      "outputs": [],
      "source": [
        "jitted = jax.jit(module.apply, static_argnames=('train',))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M16amaHdzGdK"
      },
      "outputs": [],
      "source": [
        "# Note: The model expects a batch dimension.\n",
        "predictions = jitted(\n",
        "    variables,\n",
        "    input_image[None, ...],\n",
        "    tokenized_queries[None, ...],\n",
        "    train=False)\n",
        "\n",
        "# Remove batch dimension and convert to numpy:\n",
        "predictions = jax.tree_util.tree_map(lambda x: np.array(x[0]), predictions )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_hzCvxC1sKw"
      },
      "source": [
        "# Plot predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl6Lg0jc5cKY"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZPdauOR2ZJ-"
      },
      "outputs": [],
      "source": [
        "score_threshold = 0.2\n",
        "\n",
        "logits = predictions['pred_logits'][..., :len(text_queries)]  # Remove padding.\n",
        "scores = sigmoid(np.max(logits, axis=-1))\n",
        "labels = np.argmax(predictions['pred_logits'], axis=-1)\n",
        "boxes = predictions['pred_boxes']\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
        "ax.set_axis_off()\n",
        "\n",
        "for score, box, label in zip(scores, boxes, labels):\n",
        "  if score < score_threshold:\n",
        "    continue\n",
        "  cx, cy, w, h = box\n",
        "  ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
        "          [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'r')\n",
        "  ax.text(\n",
        "      cx - w / 2,\n",
        "      cy + h / 2 + 0.015,\n",
        "      f'{text_queries[label]}: {score:1.2f}',\n",
        "      ha='left',\n",
        "      va='top',\n",
        "      color='red',\n",
        "      bbox={\n",
        "          'facecolor': 'white',\n",
        "          'edgecolor': 'red',\n",
        "          'boxstyle': 'square,pad=.3'\n",
        "      })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71-JnmJDGrMw"
      },
      "source": [
        "## Plot objectness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ8uGL9OGtyV"
      },
      "outputs": [],
      "source": [
        "top_k = 20\n",
        "objectnesses = sigmoid(predictions['objectness_logits'])\n",
        "boxes = predictions['pred_boxes']\n",
        "\n",
        "objectness_threshold = np.partition(objectnesses, -top_k)[-top_k]\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
        "ax.set_axis_off()\n",
        "\n",
        "for box, objectness in zip(boxes, objectnesses):\n",
        "  if objectness < objectness_threshold:\n",
        "    continue\n",
        "\n",
        "  cx, cy, w, h = box\n",
        "  ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
        "          [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2],\n",
        "          color='lime')\n",
        "\n",
        "  ax.text(\n",
        "    cx - w / 2 + 0.015,\n",
        "    cy + h / 2 - 0.015,\n",
        "    f'{objectness:1.2f}',\n",
        "    ha='left',\n",
        "    va='bottom',\n",
        "    color='black',\n",
        "    bbox={\n",
        "        'facecolor': 'white',\n",
        "        'edgecolor': 'lime',\n",
        "        'boxstyle': 'square,pad=.3'\n",
        "    })\n",
        "\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(1, 0)\n",
        "ax.set_title(f'Top {top_k} objects by objectness')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-hhGqbZzVfX"
      },
      "source": [
        "# Image-conditioned detection\n",
        "This section shows how to use objects detected in one image as queries on other images, instead of text strings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy51i-eZzfou"
      },
      "source": [
        "## Prepare images\n",
        "* The query object will be taken from `source_image`.\n",
        "* Then, similar objects will be detected in `target_imge`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXInmLAvzhNv"
      },
      "outputs": [],
      "source": [
        "def prepare_image(name):\n",
        "  # Load example image:\n",
        "  filename = os.path.join(skimage.data_dir, name)\n",
        "  image_uint8 = skimage_io.imread(filename)\n",
        "  image = image_uint8.astype(np.float32) / 255.0\n",
        "\n",
        "  # Pad to square with gray pixels on bottom and right:\n",
        "  h, w, _ = image.shape\n",
        "  size = max(h, w)\n",
        "  image_padded = np.pad(\n",
        "      image, ((0, size - h), (0, size - w), (0, 0)), constant_values=0.5\n",
        "  )\n",
        "\n",
        "  # Resize to model input size:\n",
        "  return skimage.transform.resize(\n",
        "      image_padded,\n",
        "      (config.dataset_configs.input_size, config.dataset_configs.input_size),\n",
        "      anti_aliasing=True,\n",
        "  )\n",
        "\n",
        "source_image = prepare_image('rocket.jpg')\n",
        "target_image = prepare_image('astronaut.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv3tBxY3_zkK"
      },
      "source": [
        "## Functions to call model components separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RjQvs1c_4j7"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "\n",
        "image_embedder = jax.jit(\n",
        "    functools.partial(\n",
        "        module.apply, variables, train=False, method=module.image_embedder\n",
        "    )\n",
        ")\n",
        "\n",
        "objectness_predictor = jax.jit(\n",
        "    functools.partial(\n",
        "        module.apply, variables, method=module.objectness_predictor\n",
        "    )\n",
        ")\n",
        "\n",
        "box_predictor = jax.jit(\n",
        "    functools.partial(module.apply, variables, method=module.box_predictor)\n",
        ")\n",
        "\n",
        "class_predictor = jax.jit(\n",
        "    functools.partial(module.apply, variables, method=module.class_predictor)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIV_4AX_0OZm"
      },
      "source": [
        "## Identify an object in the source image to use as query, and get its embedding\n",
        "\n",
        "Here, we show the top 3 predictions on the source image so that the user can select one to use as a query (we select the rocket here).\n",
        "\n",
        "To get a query embedding, it is necessary to use a box predicted by the model. We cannot directly embed a whole image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLt-t9kl1Wg7"
      },
      "outputs": [],
      "source": [
        "# Embedd images and get boxes, without text queries:\n",
        "feature_map = image_embedder(source_image[None, ...])\n",
        "\n",
        "b, h, w, d = feature_map.shape\n",
        "image_features = feature_map.reshape(b, h * w, d)\n",
        "\n",
        "objectnesses = objectness_predictor(image_features)['objectness_logits']\n",
        "\n",
        "source_boxes = box_predictor(\n",
        "    image_features=image_features, feature_map=feature_map\n",
        ")['pred_boxes']\n",
        "\n",
        "source_class_embeddings = class_predictor(image_features=image_features)[\n",
        "    'class_embeddings'\n",
        "]\n",
        "\n",
        "# Remove batch dimension\n",
        "objectnesses = np.array(objectnesses[0])\n",
        "source_boxes = np.array(source_boxes[0])\n",
        "source_class_embeddings = np.array(source_class_embeddings[0])\n",
        "\n",
        "top_k = 3\n",
        "objectnesses = sigmoid(objectnesses)\n",
        "objectness_threshold = np.partition(objectnesses, -top_k)[-top_k]\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "ax.imshow(source_image, extent=(0, 1, 1, 0))\n",
        "ax.set_axis_off()\n",
        "\n",
        "for i, (box, objectness) in enumerate(zip(source_boxes, objectnesses)):\n",
        "  if objectness < objectness_threshold:\n",
        "    continue\n",
        "\n",
        "  cx, cy, w, h = box\n",
        "  ax.plot(\n",
        "      [cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
        "      [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2],\n",
        "      color='lime',\n",
        "  )\n",
        "\n",
        "  ax.text(\n",
        "      cx - w / 2 + 0.015,\n",
        "      cy + h / 2 - 0.015,\n",
        "      f'Index {i}: {objectness:1.2f}',\n",
        "      ha='left',\n",
        "      va='bottom',\n",
        "      color='black',\n",
        "      bbox={\n",
        "          'facecolor': 'white',\n",
        "          'edgecolor': 'lime',\n",
        "          'boxstyle': 'square,pad=.3',\n",
        "      },\n",
        "  )\n",
        "\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(1, 0)\n",
        "ax.set_title(f'Top {top_k} objects by objectness')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnCPvFFz1n9o"
      },
      "outputs": [],
      "source": [
        "# Get the query embedding with the index of the selected object.\n",
        "# We're using the rocket:\n",
        "query_object_index = 1527  # Index of the rocket box above.\n",
        "query_embedding = source_class_embeddings[query_object_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HUEJvP5304U"
      },
      "source": [
        "## Get predictions for target image with the query embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DruLkz-c37Gk"
      },
      "outputs": [],
      "source": [
        "feature_map = image_embedder(target_image[None, ...])\n",
        "\n",
        "b, h, w, d = feature_map.shape\n",
        "target_boxes = box_predictor(\n",
        "    image_features=feature_map.reshape(b, h * w, d), feature_map=feature_map\n",
        ")['pred_boxes']\n",
        "\n",
        "target_class_predictions = class_predictor(\n",
        "    image_features=feature_map.reshape(b, h * w, d),\n",
        "    query_embeddings=query_embedding[None, None, ...],  # [batch, queries, d]\n",
        ")\n",
        "\n",
        "\n",
        "# Remove batch dimension and convert to numpy:\n",
        "target_boxes = np.array(target_boxes[0])\n",
        "target_logits = np.array(target_class_predictions['pred_logits'][0])\n",
        "\n",
        "top_ind = np.argmax(target_logits[:, 0], axis=0)\n",
        "score = sigmoid(target_logits[top_ind, 0])\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "ax.imshow(target_image, extent=(0, 1, 1, 0))\n",
        "ax.set_axis_off()\n",
        "\n",
        "cx, cy, w, h = target_boxes[top_ind]\n",
        "ax.plot(\n",
        "    [cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
        "    [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2],\n",
        "    color='lime',\n",
        ")\n",
        "\n",
        "ax.text(\n",
        "    cx - w / 2 + 0.015,\n",
        "    cy + h / 2 - 0.015,\n",
        "    f'Score: {score:1.2f}',\n",
        "    ha='left',\n",
        "    va='bottom',\n",
        "    color='black',\n",
        "    bbox={\n",
        "        'facecolor': 'white',\n",
        "        'edgecolor': 'lime',\n",
        "        'boxstyle': 'square,pad=.3',\n",
        "    },\n",
        ")\n",
        "\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(1, 0)\n",
        "ax.set_title(f'Closest match')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0YHQWw0R5kG"
      },
      "source": [
        "# Benchmark inference speed\n",
        "This section shows how to benchmark the inference speed of OWL-ViT. Speed and accuracy can be traded off by reducing the input resolution. This is done by truncating the position embeddings, and it works if the model was trained with heavy size augmentation and padding at the bottom and/or right of the image. This is the case for **OWL-ViT v2**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LW8_T8lTtuM"
      },
      "outputs": [],
      "source": [
        "config = configs.owl_v2_clip_b16.get_config(init_mode='canonical_checkpoint')\n",
        "\n",
        "# Replace default checkpoint with one trained on O365+VG without prompts, for\n",
        "# comparability to the literature:\n",
        "config.init_from.checkpoint_path = 'gs://scenic-bucket/owl_vit/checkpoints/owl2-b16-960-st-ngrams-ft-o365vg_925e87d'\n",
        "\n",
        "# To use variable inference resolution, patch size and native (=training) grid\n",
        "# size need to be added to the config:\n",
        "config.model.body.patch_size = int(config.model.body.variant[-2:])\n",
        "config.model.body.native_image_grid_size = (\n",
        "    config.dataset_configs.input_size // config.model.body.patch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts2dTG-sbAym"
      },
      "outputs": [],
      "source": [
        "class PredictWithTextEmbeddings(models.TextZeroShotDetectionModule):\n",
        "  \"\"\"Module that performs box prediction with precomputed query embeddings.\"\"\"\n",
        "\n",
        "  def __call__(self, image, query_embeddings):\n",
        "    feature_map = self.image_embedder(image[None, ...], False)  # Add batch dim.\n",
        "    b, h, w, d = feature_map.shape\n",
        "    image_features = feature_map.reshape(b, h * w, d)\n",
        "    boxes = self.box_predictor(\n",
        "        image_features=image_features, feature_map=feature_map\n",
        "    )['pred_boxes']\n",
        "    logits = self.class_predictor(image_features, query_embeddings[None, ...])[\n",
        "        'pred_logits'\n",
        "    ]\n",
        "    return boxes, logits\n",
        "\n",
        "\n",
        "module = PredictWithTextEmbeddings(\n",
        "    body_configs=config.model.body,\n",
        "    objectness_head_configs=config.model.objectness_head,\n",
        "    normalize=config.model.normalize,\n",
        "    box_bias=config.model.box_bias,\n",
        ")\n",
        "\n",
        "variables = module.load_variables(config.init_from.checkpoint_path)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def predict(image, query_embeddings):\n",
        "  return module.apply(variables, image, query_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpYCp8yFUetb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Get fake query embeddings for benchmarking (1203 classes):\n",
        "embed_dim = models.clip_model.CONFIGS[config.model.body.variant]['embed_dim']\n",
        "query_embeddings = jax.random.normal(jax.random.PRNGKey(0), (1203, embed_dim))\n",
        "\n",
        "# Resolutions at which to benchmark the model:\n",
        "if config.model.body.patch_size == 16:\n",
        "  sizes = [368, 400, 448, 480, 528, 576, 624, 672, 736, 784, 848, 896, 960]\n",
        "else:\n",
        "  raise ValueError(\n",
        "      'Please define image sizes for patch size:'\n",
        "      f' {config.model.body.patch_size}'\n",
        "  )\n",
        "num_trials = 5\n",
        "all_timings = {}\n",
        "for image_size in sizes:\n",
        "  print(f'Benchmarking image size: {image_size}')\n",
        "\n",
        "  # Get fake image for benchmarking:\n",
        "  image = jax.random.uniform(jax.random.PRNGKey(0), (image_size, image_size, 3))\n",
        "  timings = []\n",
        "  for i in range(num_trials + 1):  # Add 1 trial to account for compilation.\n",
        "    start_time = time.time()\n",
        "    boxes, logits = predict(image, query_embeddings)\n",
        "    _ = jax.block_until_ready((boxes, logits))\n",
        "    timings.append(time.time() - start_time)\n",
        "\n",
        "  # Store the median. Note that the first trial will always be very slow due to\n",
        "  # model commpilation:\n",
        "  all_timings[image_size] = np.median(timings)\n",
        "  print(f'FPS at resolution={image_size}: {1/all_timings[image_size]:.2f}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUtgDL0erCwm"
      },
      "source": [
        "# Models with segmentation mask head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3ljE_-irEje"
      },
      "source": [
        "## Load model with mask head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRkFxGydrCwn"
      },
      "outputs": [],
      "source": [
        "from scenic.projects.owl_vit.configs import clip_l14_with_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfVcbta9rCwn"
      },
      "outputs": [],
      "source": [
        "config = clip_l14_with_masks.get_config(init_mode='canonical_checkpoint')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bFVP-3KrCwn"
      },
      "outputs": [],
      "source": [
        "module = models.TextZeroShotDetectionModule(\n",
        "    body_configs=config.model.body,\n",
        "    mask_head_configs=config.model.mask_head,\n",
        "    normalize=config.model.normalize,\n",
        "    box_bias=config.model.box_bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDvwUne-rCwo"
      },
      "outputs": [],
      "source": [
        "variables = module.load_variables(config.init_from.checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA0qFkxPrCwo"
      },
      "outputs": [],
      "source": [
        "jitted = jax.jit(module.apply, static_argnames=('train',))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eao4T1k-rCwo"
      },
      "outputs": [],
      "source": [
        "# Resize to model input size:\n",
        "input_image = skimage.transform.resize(\n",
        "    image_padded,\n",
        "    (config.dataset_configs.input_size, config.dataset_configs.input_size),\n",
        "    anti_aliasing=True)\n",
        "\n",
        "# Note: The model expects a batch dimension.\n",
        "predictions = jitted(\n",
        "    variables,\n",
        "    input_image[None, ...],\n",
        "    tokenized_queries[None, ...],\n",
        "    train=False)\n",
        "\n",
        "# Remove batch dimension and convert to numpy:\n",
        "predictions = jax.tree_util.tree_map(lambda x: np.array(x[0]), predictions )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7uzmqqMrCwo"
      },
      "source": [
        "## Plot predictions, including masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS1_suIKrCwp"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7onch1TrCwp"
      },
      "outputs": [],
      "source": [
        "score_threshold = 0.3\n",
        "\n",
        "logits = predictions['pred_logits'][..., :len(text_queries)]  # Remove padding.\n",
        "scores = sigmoid(np.max(logits, axis=-1))\n",
        "labels = np.argmax(predictions['pred_logits'], axis=-1)\n",
        "boxes = predictions['pred_boxes']\n",
        "\n",
        "masks = [None] * len(boxes)\n",
        "if 'pred_masks' in predictions:\n",
        "  masks = sigmoid(predictions['pred_masks'])\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
        "ax.set_axis_off()\n",
        "\n",
        "for score, box, label, mask in zip(scores, boxes, labels, masks):\n",
        "  if score < score_threshold:\n",
        "    continue\n",
        "  cx, cy, w, h = box\n",
        "  ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
        "          [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'r')\n",
        "\n",
        "  if mask is not None:\n",
        "    mask_img = plt.cm.viridis(mask)\n",
        "    mask_img[..., -1] = (mask > 0.5) * 0.8\n",
        "    extent = np.array((cx - w / 2, cx + w / 2, cy + h / 2, cy - h / 2))\n",
        "    ax.imshow(mask_img, extent=np.clip(extent, 0, 1))\n",
        "\n",
        "  ax.text(\n",
        "      cx - w / 2,\n",
        "      cy + h / 2 + 0.015,\n",
        "      f'{text_queries[label]}: {score:1.2f}',\n",
        "      ha='left',\n",
        "      va='top',\n",
        "      color='red',\n",
        "      bbox={\n",
        "          'facecolor': 'white',\n",
        "          'edgecolor': 'red',\n",
        "          'boxstyle': 'square,pad=.3'\n",
        "      })\n",
        "\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(1, 0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}